{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_character_generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tMdkCn_5lI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-wOJ3iS6DjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "005f3a45-d9d4-4498-aa6b-c2d8dcd49786"
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "print(train_on_gpu)\n",
        "with open('assets/jane-eyre.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "text[:100]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Jane Eyre\\n\\nthat parent of crime—an insult to piety, that regent of God\\non earth. I would suggest to '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEvof6P06nWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "b6e93cb1-7622-4f98-da5e-0f1a9c5c1a08"
      },
      "source": [
        "text = text.lower()\n",
        "chars = tuple(set(text))\n",
        "int_to_char = dict(enumerate(chars))\n",
        "char_to_int = {ch: i for i, ch in int_to_char.items()}\n",
        "print(chars, len(chars))\n",
        "encoded = np.array([char_to_int[char] for char in text])\n",
        "encoded[:100]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('8', 't', '0', 'x', ';', 'q', '5', 'p', '(', 'm', 'z', '!', ')', '7', ',', '.', ':', '’', '6', 'l', '&', 'd', 'w', 'u', 'e', '\\n', 'a', '3', 'o', 'v', 'b', ' ', '\\x0c', 'r', '‘', '—', 'f', 's', '-', '4', '1', 'j', 'h', '2', 'y', 'i', '?', '9', 'n', 'c', 'g', 'k') 52\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([41, 26, 48, 24, 31, 24, 44, 33, 24, 25, 25,  1, 42, 26,  1, 31,  7,\n",
              "       26, 33, 24, 48,  1, 31, 28, 36, 31, 49, 33, 45,  9, 24, 35, 26, 48,\n",
              "       31, 45, 48, 37, 23, 19,  1, 31,  1, 28, 31,  7, 45, 24,  1, 44, 14,\n",
              "       31,  1, 42, 26,  1, 31, 33, 24, 50, 24, 48,  1, 31, 28, 36, 31, 50,\n",
              "       28, 21, 25, 28, 48, 31, 24, 26, 33,  1, 42, 15, 31, 45, 31, 22, 28,\n",
              "       23, 19, 21, 31, 37, 23, 50, 50, 24, 37,  1, 31,  1, 28, 31])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkQi235X7ON7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_vector(arr, n_labels):\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
        "\n",
        "    one_hot = one_hot.reshape((*arr.shape,n_labels))\n",
        "\n",
        "    return one_hot"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLCa4Zpr9VGk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "53ada9e0-bfbe-46b4-c28d-db32e77a9065"
      },
      "source": [
        "one_hot_vector(encoded[:3], len(chars))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVs8Q1rS9iql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, seq_len, batch_size):\n",
        "    batch_total = seq_len * batch_size\n",
        "    total_batches = len(arr)//batch_total\n",
        "    arr = arr[:batch_total*total_batches]\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    for n in range(0, arr.shape[1], seq_len):\n",
        "        x = arr[:, n:n+seq_len]\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_len]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WFNyvwaBcEG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "2a63a4a6-4967-4c7a-cfdf-ffacaef39697"
      },
      "source": [
        "batches = get_batches(encoded, 10, 50)\n",
        "x, y = next(batches)\n",
        "\n",
        "print(\"X: \", x[:10])\n",
        "print(\"Y: \", y[:10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X:  [[41 26 48 24 31 24 44 33 24 25]\n",
            " [33 24  7 24 26  1 24 21 31  1]\n",
            " [31 28 30 19 45 50 24 21 31  1]\n",
            " [ 3  7 33 24 37 37 45 29 24 31]\n",
            " [51 14 25 22 42 45 49 42 31 42]\n",
            " [ 4 31 26 31 30 33 45 24 36 31]\n",
            " [14 31 26 48 21 31  1 28 31 22]\n",
            " [31 36 28 33 31  1 42 24 31  1]\n",
            " [31 49 33 28 22 21 24 21 31 37]\n",
            " [ 9 30 24 33 24 21 31 21 24 37]]\n",
            "Y:  [[26 48 24 31 24 44 33 24 25 25]\n",
            " [24  7 24 26  1 24 21 31  1 42]\n",
            " [28 30 19 45 50 24 21 31  1 28]\n",
            " [ 7 33 24 37 37 45 29 24 31 37]\n",
            " [14 25 22 42 45 49 42 31 42 26]\n",
            " [31 26 31 30 33 45 24 36 31 24]\n",
            " [31 26 48 21 31  1 28 31 22 42]\n",
            " [36 28 33 31  1 42 24 31  1 33]\n",
            " [49 33 28 22 21 24 21 31 37 49]\n",
            " [30 24 33 24 21 31 21 24 37 49]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1_jwKhWCAlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelLSTM(nn.Module):\n",
        "    def __init__(self, unique_chars, n_hidden, n_layers, drop_prob = 0.5, lr = 0.005):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_layers = n_layers\n",
        "        self.drop_prob = drop_prob\n",
        "        self.lr = lr\n",
        "\n",
        "        self.chars = unique_chars\n",
        "        self.int_to_char = dict(enumerate(self.chars))\n",
        "        self.char_to_int = {ch:ii for ii, ch in int_to_char.items()}\n",
        "\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, batch_first = True, dropout = drop_prob)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=self.drop_prob)\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "\n",
        "        r_out, hidden = self.lstm(x, hidden)\n",
        "        out = self.dropout(r_out)\n",
        "\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKrhxEdUJEQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, data, checkpoint, epoches = 50, batch_size = 130, seq_len = 105, lr = 0.005, print_step = 50, clip = 5, data_frac = 0.2):\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    val_idx = int(len(data) * (1- data_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx: ]\n",
        "\n",
        "    model.cuda()\n",
        "    n_chars = len(model.chars)\n",
        "    counter = 0\n",
        "    val_loss_min = np.Inf\n",
        "    for epoch in range(epoches + 1):\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, seq_len, batch_size):\n",
        "            counter += 1\n",
        "            x = one_hot_vector(x, n_chars)\n",
        "            x, y = torch.from_numpy(x).cuda(), torch.from_numpy(y).cuda()\n",
        "\n",
        "            hidden = tuple([h.data for h in hidden])\n",
        "            optimizer.zero_grad()\n",
        "            output, hidden = model(x, hidden)\n",
        "            loss = criterion(output, y.view(batch_size*seq_len).long())\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            if counter%print_step == 0:\n",
        "                model.eval()\n",
        "                val_hidden = model.init_hidden(batch_size)\n",
        "                val_loss = []\n",
        "\n",
        "                for x, y in get_batches(val_data, seq_len, batch_size):\n",
        "                    x = one_hot_vector(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x).cuda(), torch.from_numpy(y).cuda()\n",
        "\n",
        "                    hidden = tuple([h.data for h in hidden])\n",
        "                    output, val_hidden = model(x, hidden)\n",
        "                    v_loss = criterion(output, y.view(batch_size * seq_len).long())\n",
        "                    val_loss.append(v_loss.item())\n",
        "                val_loss_mean = np.mean(val_loss)\n",
        "                if val_loss_min > val_loss_mean:\n",
        "                    print(\"SAving the model\")\n",
        "                    checkpoint['state_dict'] = model.state_dict()\n",
        "                    val_loss_min = val_loss_mean\n",
        "                model.train()\n",
        "                print(\"Epoch: {}/{}\\tstep: {}\\tloss: {:.6f}\\tval_loss: {:.6f}\".format(epoch, epoches, counter, loss, val_loss_mean))\n",
        "    return checkpoint"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOwU7WJ7oq9Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "14992b6c-24c0-4c23-806b-9baecb9e405e"
      },
      "source": [
        "n_hidden = 500\n",
        "n_layers = 2\n",
        "\n",
        "model = ModelLSTM(chars, n_hidden, n_layers)\n",
        "print(model)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ModelLSTM(\n",
            "  (lstm): LSTM(52, 500, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=500, out_features=52, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M61cxJNDpESs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47c67b14-8f2c-4275-ad28-08ffb2e6313c"
      },
      "source": [
        "batch_size = 130\n",
        "seq_len = 105\n",
        "epoches = 50\n",
        "checkpoint = {}\n",
        "checkpoint = train(model, encoded, checkpoint, epoches, batch_size, seq_len)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SAving the model\n",
            "Epoch: 0/50\tstep: 50\tloss: 3.142976\tval_loss: 3.131405\n",
            "SAving the model\n",
            "Epoch: 1/50\tstep: 100\tloss: 3.051948\tval_loss: 3.045013\n",
            "SAving the model\n",
            "Epoch: 2/50\tstep: 150\tloss: 2.847439\tval_loss: 2.841233\n",
            "SAving the model\n",
            "Epoch: 3/50\tstep: 200\tloss: 2.478062\tval_loss: 2.426440\n",
            "SAving the model\n",
            "Epoch: 4/50\tstep: 250\tloss: 2.299975\tval_loss: 2.242040\n",
            "SAving the model\n",
            "Epoch: 4/50\tstep: 300\tloss: 2.208694\tval_loss: 2.116657\n",
            "SAving the model\n",
            "Epoch: 5/50\tstep: 350\tloss: 2.109591\tval_loss: 2.024257\n",
            "SAving the model\n",
            "Epoch: 6/50\tstep: 400\tloss: 2.028883\tval_loss: 1.951760\n",
            "SAving the model\n",
            "Epoch: 7/50\tstep: 450\tloss: 1.957888\tval_loss: 1.893900\n",
            "SAving the model\n",
            "Epoch: 8/50\tstep: 500\tloss: 1.921939\tval_loss: 1.845848\n",
            "SAving the model\n",
            "Epoch: 9/50\tstep: 550\tloss: 1.882979\tval_loss: 1.804886\n",
            "SAving the model\n",
            "Epoch: 9/50\tstep: 600\tloss: 1.878345\tval_loss: 1.763526\n",
            "SAving the model\n",
            "Epoch: 10/50\tstep: 650\tloss: 1.813301\tval_loss: 1.728702\n",
            "SAving the model\n",
            "Epoch: 11/50\tstep: 700\tloss: 1.763229\tval_loss: 1.701241\n",
            "SAving the model\n",
            "Epoch: 12/50\tstep: 750\tloss: 1.732608\tval_loss: 1.671369\n",
            "SAving the model\n",
            "Epoch: 13/50\tstep: 800\tloss: 1.721875\tval_loss: 1.650648\n",
            "SAving the model\n",
            "Epoch: 14/50\tstep: 850\tloss: 1.701424\tval_loss: 1.631385\n",
            "SAving the model\n",
            "Epoch: 14/50\tstep: 900\tloss: 1.715778\tval_loss: 1.606808\n",
            "SAving the model\n",
            "Epoch: 15/50\tstep: 950\tloss: 1.657075\tval_loss: 1.592081\n",
            "SAving the model\n",
            "Epoch: 16/50\tstep: 1000\tloss: 1.622141\tval_loss: 1.578881\n",
            "SAving the model\n",
            "Epoch: 17/50\tstep: 1050\tloss: 1.597503\tval_loss: 1.564176\n",
            "SAving the model\n",
            "Epoch: 18/50\tstep: 1100\tloss: 1.596022\tval_loss: 1.549681\n",
            "SAving the model\n",
            "Epoch: 19/50\tstep: 1150\tloss: 1.579390\tval_loss: 1.542553\n",
            "SAving the model\n",
            "Epoch: 19/50\tstep: 1200\tloss: 1.617874\tval_loss: 1.526352\n",
            "SAving the model\n",
            "Epoch: 20/50\tstep: 1250\tloss: 1.553282\tval_loss: 1.516863\n",
            "SAving the model\n",
            "Epoch: 21/50\tstep: 1300\tloss: 1.530682\tval_loss: 1.514258\n",
            "SAving the model\n",
            "Epoch: 22/50\tstep: 1350\tloss: 1.497262\tval_loss: 1.505279\n",
            "SAving the model\n",
            "Epoch: 23/50\tstep: 1400\tloss: 1.521908\tval_loss: 1.496140\n",
            "SAving the model\n",
            "Epoch: 24/50\tstep: 1450\tloss: 1.506484\tval_loss: 1.486871\n",
            "SAving the model\n",
            "Epoch: 24/50\tstep: 1500\tloss: 1.547408\tval_loss: 1.479134\n",
            "SAving the model\n",
            "Epoch: 25/50\tstep: 1550\tloss: 1.474056\tval_loss: 1.472355\n",
            "Epoch: 26/50\tstep: 1600\tloss: 1.459896\tval_loss: 1.473858\n",
            "SAving the model\n",
            "Epoch: 27/50\tstep: 1650\tloss: 1.427719\tval_loss: 1.467880\n",
            "SAving the model\n",
            "Epoch: 28/50\tstep: 1700\tloss: 1.446744\tval_loss: 1.462268\n",
            "SAving the model\n",
            "Epoch: 29/50\tstep: 1750\tloss: 1.428382\tval_loss: 1.459271\n",
            "SAving the model\n",
            "Epoch: 29/50\tstep: 1800\tloss: 1.482177\tval_loss: 1.453472\n",
            "SAving the model\n",
            "Epoch: 30/50\tstep: 1850\tloss: 1.416161\tval_loss: 1.450426\n",
            "SAving the model\n",
            "Epoch: 31/50\tstep: 1900\tloss: 1.409392\tval_loss: 1.447731\n",
            "SAving the model\n",
            "Epoch: 32/50\tstep: 1950\tloss: 1.374428\tval_loss: 1.446946\n",
            "SAving the model\n",
            "Epoch: 33/50\tstep: 2000\tloss: 1.399436\tval_loss: 1.446295\n",
            "SAving the model\n",
            "Epoch: 34/50\tstep: 2050\tloss: 1.387049\tval_loss: 1.443151\n",
            "SAving the model\n",
            "Epoch: 34/50\tstep: 2100\tloss: 1.435046\tval_loss: 1.441915\n",
            "SAving the model\n",
            "Epoch: 35/50\tstep: 2150\tloss: 1.375038\tval_loss: 1.439583\n",
            "SAving the model\n",
            "Epoch: 36/50\tstep: 2200\tloss: 1.356040\tval_loss: 1.439554\n",
            "Epoch: 37/50\tstep: 2250\tloss: 1.326729\tval_loss: 1.442162\n",
            "SAving the model\n",
            "Epoch: 38/50\tstep: 2300\tloss: 1.354544\tval_loss: 1.438232\n",
            "SAving the model\n",
            "Epoch: 39/50\tstep: 2350\tloss: 1.340723\tval_loss: 1.437415\n",
            "SAving the model\n",
            "Epoch: 39/50\tstep: 2400\tloss: 1.398048\tval_loss: 1.433753\n",
            "Epoch: 40/50\tstep: 2450\tloss: 1.342593\tval_loss: 1.443664\n",
            "Epoch: 41/50\tstep: 2500\tloss: 1.326515\tval_loss: 1.439708\n",
            "Epoch: 42/50\tstep: 2550\tloss: 1.296651\tval_loss: 1.436807\n",
            "Epoch: 43/50\tstep: 2600\tloss: 1.325710\tval_loss: 1.436062\n",
            "Epoch: 44/50\tstep: 2650\tloss: 1.307518\tval_loss: 1.441884\n",
            "Epoch: 44/50\tstep: 2700\tloss: 1.358493\tval_loss: 1.440190\n",
            "Epoch: 45/50\tstep: 2750\tloss: 1.306184\tval_loss: 1.442203\n",
            "Epoch: 46/50\tstep: 2800\tloss: 1.296001\tval_loss: 1.446680\n",
            "Epoch: 47/50\tstep: 2850\tloss: 1.255935\tval_loss: 1.448368\n",
            "Epoch: 48/50\tstep: 2900\tloss: 1.294764\tval_loss: 1.445222\n",
            "Epoch: 49/50\tstep: 2950\tloss: 1.282892\tval_loss: 1.448271\n",
            "Epoch: 49/50\tstep: 3000\tloss: 1.335369\tval_loss: 1.444160\n",
            "Epoch: 50/50\tstep: 3050\tloss: 1.277537\tval_loss: 1.449449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYOUg8V0pl_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'lstm_char.net'\n",
        "\n",
        "checkpoint['n_hidden'] = model.n_hidden\n",
        "checkpoint['n_layers'] = model.n_layers\n",
        "checkpoint['unique_chars'] = model.chars\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8-m1DdlxkqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zWIZr7JwR6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, char, hidden = None, top_k = None):\n",
        "    x = np.array([[model.char_to_int[char]]])\n",
        "    x = one_hot_vector(x, len(model.chars))\n",
        "    inputs = torch.from_numpy(x).cuda()\n",
        "\n",
        "    hidden = tuple([h for h in hidden])\n",
        "    out, hidden = model(inputs, hidden)\n",
        "\n",
        "    p= F.softmax(out, dim = 1).data\n",
        "    p = p.cpu()\n",
        "\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(model.chars))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p = p/p.sum())\n",
        "\n",
        "    return model.int_to_char[char], hidden"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBtLwLPqyTlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(model, size, prime=\"it was\", top_k = None):\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    chars = [ch for ch in prime]\n",
        "    hidden = model.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, hidden = predict(model, ch, hidden, top_k=top_k)\n",
        "    chars.append(char)\n",
        "\n",
        "    for ii in range(size):\n",
        "        char, hidden = predict(model, chars[-1], hidden, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCJzFxNEzPrV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f2eb456-9ab4-4361-edc3-d03336c39b87"
      },
      "source": [
        "with open('lstm_char.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "\n",
        "loaded = ModelLSTM(checkpoint['unique_chars'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTqhEDp20Gmj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "11bb3eca-ae01-44c9-da74-08ef46be55c0"
      },
      "source": [
        "print(sample(loaded, 2000, top_k=5, prime=\"that guy loves her\"))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "that guy loves here and second\n",
            "his cheek and class; i wish to say i see that they were at all rochester, that half thought to assume to see, seemed to\n",
            "him in silence: the character of the subject i was sure, as i had better. the\n",
            "fine proprietor was to be conscientively on a servant. in\n",
            "short that i was often by the pity-moon supposed. i saw a girl watched\n",
            "the last to me, sometimes and i continued. then stopped with the\n",
            "ploom was the first to be called over the casements: at this thing i had been ready to\n",
            "strength to the fire and country, but she liked a lady in miss miller and mind, at least, the toil, such could be sure,\n",
            "where the desire of the face of me. he had been too shrank as a strange trees with\n",
            "that candle and serve to an expression of those way, that the closet\n",
            "i had seen all was, and she went up to as its fierle. he could not see an and shape that this subject, i suppose,\n",
            "was indifficult to be too like herself. i was to be\n",
            "thinking and could see this thought of a face. i lived here with a second conversation on the\n",
            "constitution of the fire.\n",
            "‘i do not allow them. i have not so surprise is, and\n",
            "they are so still abbot: i should never help to the passanance, and and my soul,\n",
            "would not set; but to the ladies, in the piece of miss\n",
            "temple should not fell bright to silver, stringing it to the distance of him by a care of me; and i was\n",
            "naturally between the chief of me in an accomplishments against the\n",
            "districts of the charm and stool.\n",
            "my stand at a cheese, which i sent to the careful sigus on the\n",
            "servants of the cold and satudal servants and antipant that, and the chamber-back of the face. as i\n",
            "saw it a contrary, to seek hand was a shade of a patient; but still\n",
            "she saw me. i had not forgetting my facultion, and\n",
            "chood, and allow, wind the flood, as they are still a morning that\n",
            "worse thoughts in a fair time in a party was they shore in my countenance, i had been the fear\n",
            "that they are sure you are the seal of the\n",
            "window-servants in a malestation; stranger in a steady and\n",
            "bridinace \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K8s-mIH0h7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}